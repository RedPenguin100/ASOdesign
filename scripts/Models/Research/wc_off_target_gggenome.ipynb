{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-17T22:24:54.823064Z",
     "start_time": "2025-09-17T22:24:46.125339Z"
    }
   },
   "source": [
    "from scripts.data_genertion.consts import *\n",
    "from scripts.features.feature_extraction import load_all_features\n",
    "\n",
    "main_df = load_all_features()\n",
    "main_df[SEQUENCE] = main_df[SEQUENCE].astype(str)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T22:24:55.090428Z",
     "start_time": "2025-09-17T22:24:54.839912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from asodesigner.read_human_genome import get_locus_to_data_dict\n",
    "import pickle\n",
    "from asodesigner.consts import CACHE_DIR\n",
    "\n",
    "genes_u = ['HIF1A', 'APOL1', 'YAP1', 'SOD1', 'SNCA', 'IRF4', 'KRAS', 'KLKB1', 'SNHG14', 'DGAT2', 'IRF5', 'HTRA1',\n",
    "           'MYH7', 'MALAT1', 'HSD17B13']\n",
    "cache_path = CACHE_DIR / 'gene_to_data_simple_cache.pickle'\n",
    "if not cache_path.exists():\n",
    "    gene_to_data = get_locus_to_data_dict(include_introns=True, gene_subset=genes_u)\n",
    "    with open(cache_path, 'wb') as f:\n",
    "        pickle.dump(gene_to_data, f)\n",
    "else:\n",
    "    with open(cache_path, 'rb') as f:\n",
    "        gene_to_data = pickle.load(f)"
   ],
   "id": "23847d4556b4b520",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T22:25:10.278743Z",
     "start_time": "2025-09-17T22:24:55.125502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scripts.data_genertion.data_handling import get_populated_df_with_structure_features\n",
    "\n",
    "main_df = get_populated_df_with_structure_features(main_df, genes_u, gene_to_data)"
   ],
   "id": "6e6e1fa4dc839646",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T22:25:10.329962Z",
     "start_time": "2025-09-17T22:25:10.286260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "main_df = main_df[main_df[SENSE_START] != -1] # only found ASOs\n",
    "main_df[SEQUENCE]\n"
   ],
   "id": "41c30d2343ac2bb6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        GCTAAAACAAATGCTA\n",
       "1        TATAATGGTGAATATC\n",
       "2        GCATGAAGATTTCTGG\n",
       "3        GGTGAATATCTTCAAA\n",
       "4        CACTTGTACTAGTATG\n",
       "               ...       \n",
       "34760    GTTATGAAATTATTGG\n",
       "34761    ATTCTATTAGAGGGCT\n",
       "34762    GCTTTAAACTCAGGTG\n",
       "34763    CGTCAATATATTCTTT\n",
       "34764    TTTTGTAAGTGCAACC\n",
       "Name: Sequence, Length: 29987, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-17T22:25:10.340166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pip install requests\n",
    "import math, time, threading, urllib.parse, requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "UA = {\"User-Agent\": \"python-requests gggenome/greedy\"}\n",
    "\n",
    "def _ggg_hits_leq_json(seq, k, db=\"hg38\", timeout=60, retries=2):\n",
    "    \"\"\"Count hits with <=k mismatches via GGGenome JSON; fallback to CSV if needed.\"\"\"\n",
    "    s = str(seq).upper().replace(\"U\", \"T\")\n",
    "    q = urllib.parse.quote(s)\n",
    "    url_json = f\"https://gggenome.dbcls.jp/{db}/{k}/nogap/{q}.json\"\n",
    "    url_csv  = f\"https://gggenome.dbcls.jp/{db}/{k}/nogap/{q}.csv?download\"\n",
    "\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            r = requests.get(url_json, headers=UA, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            try:\n",
    "                data = r.json()\n",
    "            except ValueError:\n",
    "                raise RuntimeError(\"JSON parse failed\")\n",
    "            if isinstance(data, list):\n",
    "                return len(data)\n",
    "            if isinstance(data, dict):\n",
    "                if \"results\" in data and isinstance(data[\"results\"], list): return len(data[\"results\"])\n",
    "                if \"hits\" in data and isinstance(data[\"hits\"], list):       return len(data[\"hits\"])\n",
    "                return sum(len(v) for v in data.values() if isinstance(v, list))\n",
    "            return 0\n",
    "        except Exception:\n",
    "            # greedy CSV fallback\n",
    "            try:\n",
    "                r2 = requests.get(url_csv, headers=UA, timeout=timeout)\n",
    "                r2.raise_for_status()\n",
    "                return sum(1 for ln in r2.text.splitlines() if ln and not ln.startswith(\"#\"))\n",
    "            except Exception:\n",
    "                if attempt < retries:\n",
    "                    continue\n",
    "                return 0\n",
    "    return 0\n",
    "\n",
    "def _d123_for_sequence(seq, db=\"hg38\"):\n",
    "    s = str(seq).upper().replace(\"U\", \"T\")\n",
    "    if not s:\n",
    "        return (s, 0, 0, 0)\n",
    "    L = len(s)\n",
    "    k_allowed = max(0, math.floor(0.25 * L))  # GGGenome cap\n",
    "    k0 = _ggg_hits_leq_json(s, 0, db=db)\n",
    "    k1 = _ggg_hits_leq_json(s, 1, db=db) if k_allowed >= 1 else 0\n",
    "    k2 = _ggg_hits_leq_json(s, 2, db=db) if k_allowed >= 2 else 0\n",
    "    k3 = _ggg_hits_leq_json(s, 3, db=db) if k_allowed >= 3 else 0\n",
    "    d1 = max(0, k1 - k0)\n",
    "    d2 = max(0, k2 - k1)\n",
    "    d3 = max(0, k3 - k2)\n",
    "    return (s, d1, d2, d3)\n",
    "\n",
    "def add_gggenome_d123(main_df, seq_col=\"SEQUENCE\", db=\"hg38\", *, max_workers=32, print_every=10):\n",
    "    seqs = (main_df[seq_col].astype(str).str.upper().str.replace(\"U\", \"T\", regex=False))\n",
    "    uniq = seqs.dropna().unique().tolist()\n",
    "    N = len(uniq)\n",
    "    print(f\"[GGG] Unique sequences: {N} | db={db} | workers={max_workers}\")\n",
    "\n",
    "    cache = {}\n",
    "    lock = threading.Lock()\n",
    "    t0 = time.perf_counter()\n",
    "    errs = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = {ex.submit(_d123_for_sequence, s, db): s for s in uniq}\n",
    "        done = 0\n",
    "        for fut in as_completed(futs):\n",
    "            s = futs[fut]\n",
    "            try:\n",
    "                s_key, d1, d2, d3 = fut.result()\n",
    "            except Exception:\n",
    "                d1 = d2 = d3 = 0\n",
    "                with lock:\n",
    "                    errs += 1\n",
    "            with lock:\n",
    "                cache[s] = (d1, d2, d3)\n",
    "                done += 1\n",
    "                if (done == 1) or (done % print_every == 0) or (done == N):\n",
    "                    elapsed = time.perf_counter() - t0\n",
    "                    rps = done / elapsed if elapsed > 0 else 0.0\n",
    "                    print(f\"[GGG] {done}/{N} cached | ~{rps:.1f} seq/s | errors={errs}\")\n",
    "\n",
    "    main_df[\"ggg_d1\"] = seqs.map(lambda s: cache.get(s, (0, 0, 0))[0])\n",
    "    main_df[\"ggg_d2\"] = seqs.map(lambda s: cache.get(s, (0, 0, 0))[1])\n",
    "    main_df[\"ggg_d3\"] = seqs.map(lambda s: cache.get(s, (0, 0, 0))[2])\n",
    "\n",
    "    print(f\"[GGG] Finished in {time.perf_counter() - t0:.1f}s. Added columns: ggg_d1, ggg_d2, ggg_d3\")\n",
    "    return main_df\n",
    "\n",
    "# --- usage ---\n",
    "# main_df = main_df[main_df[SENSE_START] != -1]\n",
    "main_df = add_gggenome_d123(main_df, seq_col=SEQUENCE, db=\"hg38\", max_workers=100, print_every=2)\n"
   ],
   "id": "e0a45d92f8a08ec2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GGG] Unique sequences: 14123 | db=hg38 | workers=100\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scripts.util.print import print_correlations\n",
    "\n",
    "print_correlations(main_df, 'ggg_d1', INHIBITION)\n",
    "print_correlations(main_df, 'ggg_d2', INHIBITION)\n",
    "print_correlations(main_df, 'ggg_d3', INHIBITION)"
   ],
   "id": "435a804ee091c4e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scripts.features.feature_extraction import save_feature\n",
    "save_feature(main_df, 'ggg_d1')\n",
    "save_feature(main_df, 'ggg_d2')\n",
    "save_feature(main_df, 'ggg_d3')"
   ],
   "id": "1e9774849447053b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# main_df = main_df[main_df[SENSE_START] != -1]\n",
    "# main_df[\"ggg_d1\"] = main_df[SEQUENCE].map(_d1_cached)"
   ],
   "id": "76bc0fb3de85334e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "main_df[main_df['ggg_d3'] != 0]['ggg_d3',]",
   "id": "c734d5e1579f46d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pip install requests\n",
    "import math, time, threading, urllib.parse, requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "UA = {\"User-Agent\": \"python-requests gggenome/greedy\"}\n",
    "\n",
    "def _ggg_hits_leq_json(seq, k, db=\"refseq\", timeout=120, retries=2):\n",
    "    \"\"\"\n",
    "    Count hits with <=k mismatches via GGGenome JSON; fallback to CSV if needed.\n",
    "    Prints errors but returns 0 on failure so the pipeline continues.\n",
    "    \"\"\"\n",
    "    s = str(seq).upper().replace(\"U\", \"T\")\n",
    "    if not s:\n",
    "        return 0\n",
    "    q = urllib.parse.quote(s)\n",
    "    url_json = f\"https://gggenome.dbcls.jp/{db}/{k}/nogap/{q}.json\"\n",
    "    url_csv  = f\"https://gggenome.dbcls.jp/{db}/{k}/nogap/{q}.csv?download\"\n",
    "\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            r = requests.get(url_json, headers=UA, timeout=timeout)\n",
    "            r.raise_for_status()\n",
    "            try:\n",
    "                data = r.json()\n",
    "            except ValueError as e:\n",
    "                print(f\"[ERROR] JSON parse failed for seq={s}, k={k}, url={url_json}: {e}\")\n",
    "                raise\n",
    "\n",
    "            if isinstance(data, list):\n",
    "                return len(data)\n",
    "            if isinstance(data, dict):\n",
    "                if \"results\" in data and isinstance(data[\"results\"], list):\n",
    "                    return len(data[\"results\"])\n",
    "                if \"hits\" in data and isinstance(data[\"hits\"], list):\n",
    "                    return len(data[\"hits\"])\n",
    "                return sum(len(v) for v in data.values() if isinstance(v, list))\n",
    "            return 0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] JSON failed (attempt {attempt+1}) for seq={s}, k={k}, url={url_json}: {e}\")\n",
    "            # Try CSV fallback\n",
    "            try:\n",
    "                r2 = requests.get(url_csv, headers=UA, timeout=timeout)\n",
    "                r2.raise_for_status()\n",
    "                return sum(1 for ln in r2.text.splitlines() if ln and not ln.startswith(\"#\"))\n",
    "            except Exception as e2:\n",
    "                print(f\"[ERROR] CSV fallback failed for seq={s}, k={k}, url={url_csv}: {e2}\")\n",
    "                if attempt < retries:\n",
    "                    continue\n",
    "                return 0\n",
    "    return 0\n",
    "\n",
    "def _k_counts_for_sequence(seq, db=\"refseq\"):\n",
    "    \"\"\"Return cumulative counts (k0..k3) and derived d1..d3 for a sequence.\"\"\"\n",
    "    s = str(seq).upper().replace(\"U\", \"T\")\n",
    "    if not s:\n",
    "        return (s, 0, 0, 0, 0, 0, 0)\n",
    "    L = len(s)\n",
    "    k_allowed = max(0, math.floor(0.25 * L))  # GGGenome cap\n",
    "    k0 = _ggg_hits_leq_json(s, 0, db=db)\n",
    "    k1 = _ggg_hits_leq_json(s, 1, db=db) if k_allowed >= 1 else k0\n",
    "    k2 = _ggg_hits_leq_json(s, 2, db=db) if k_allowed >= 2 else k1\n",
    "    k3 = _ggg_hits_leq_json(s, 3, db=db) if k_allowed >= 3 else k2\n",
    "    d1 = max(0, k1 - k0)\n",
    "    d2 = max(0, k2 - k1)\n",
    "    d3 = max(0, k3 - k2)\n",
    "    return (s, k0, k1, k2, k3, d1, d2, d3)\n",
    "\n",
    "def add_gggenome_tx_d123(\n",
    "    main_df, seq_col=\"SEQUENCE\", db=\"refseq\", *,\n",
    "    max_workers=32, print_every=10, return_k=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Add transcriptome off-target bins (nogap mismatches).\n",
    "    tx_d1, tx_d2, tx_d3 = hits with exactly 1/2/3 mismatches.\n",
    "    If return_k=True, also adds tx_k0..tx_k3.\n",
    "    \"\"\"\n",
    "    seqs = main_df[seq_col].astype(str).str.upper().str.replace(\"U\", \"T\", regex=False)\n",
    "    uniq = seqs.dropna().unique().tolist()\n",
    "    N = len(uniq)\n",
    "    print(f\"[GGG-TX] Unique sequences: {N} | db={db} | workers={max_workers}\")\n",
    "\n",
    "    cache = {}\n",
    "    lock = threading.Lock()\n",
    "    t0 = time.perf_counter()\n",
    "    errs = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        futs = {ex.submit(_k_counts_for_sequence, s, db): s for s in uniq}\n",
    "        done = 0\n",
    "        for fut in as_completed(futs):\n",
    "            s = futs[fut]\n",
    "            try:\n",
    "                s_key, k0, k1, k2, k3, d1, d2, d3 = fut.result()\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Worker failed for seq={s}: {e}\")\n",
    "                k0 = k1 = k2 = k3 = d1 = d2 = d3 = 0\n",
    "                with lock:\n",
    "                    errs += 1\n",
    "            with lock:\n",
    "                cache[s] = (k0, k1, k2, k3, d1, d2, d3)\n",
    "                done += 1\n",
    "                if (done == 1) or (done % print_every == 0) or (done == N):\n",
    "                    elapsed = time.perf_counter() - t0\n",
    "                    rps = done / elapsed if elapsed > 0 else 0.0\n",
    "                    print(f\"[GGG-TX] {done}/{N} cached | ~{rps:.1f} seq/s | errors={errs}\")\n",
    "\n",
    "    main_df[\"tx_d1\"] = seqs.map(lambda s: cache.get(s, (0,0,0,0,0,0,0))[4])\n",
    "    main_df[\"tx_d2\"] = seqs.map(lambda s: cache.get(s, (0,0,0,0,0,0,0))[5])\n",
    "    main_df[\"tx_d3\"] = seqs.map(lambda s: cache.get(s, (0,0,0,0,0,0,0))[6])\n",
    "\n",
    "    if return_k:\n",
    "        main_df[\"tx_k0\"] = seqs.map(lambda s: cache.get(s, (0,0,0,0,0,0,0))[0])\n",
    "        main_df[\"tx_k1\"] = seqs.map(lambda s: cache.get(s, (0,0,0,0,0,0,0))[1])\n",
    "        main_df[\"tx_k2\"] = seqs.map(lambda s: cache.get(s, (0,0,0,0,0,0,0))[2])\n",
    "        main_df[\"tx_k3\"] = seqs.map(lambda s: cache.get(s, (0,0,0,0,0,0,0))[3])\n",
    "\n",
    "    finish = time.perf_counter() - t0\n",
    "    added = \"tx_d1, tx_d2, tx_d3\" + (\", tx_k0..tx_k3\" if return_k else \"\")\n",
    "    print(f\"[GGG-TX] Finished in {finish:.1f}s. Added columns: {added}\")\n",
    "    return main_df\n",
    "\n",
    "# --- usage ---\n",
    "# main_df = main_df[main_df[SENSE_START] != -1]\n",
    "# RefSeq transcripts (default):\n",
    "main_df = add_gggenome_tx_d123(main_df, seq_col=SEQUENCE, db=\"refseq\", max_workers=256, print_every=2, return_k=True)\n",
    "# Or GENCODE transcripts:\n",
    "# main_df = add_gggenome_tx_d123(main_df, seq_col=SEQUENCE, db=\"GENCODE_47\", max_workers=256, print_every=2, return_k=True)\n"
   ],
   "id": "32c6af9cd28493db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "67769655cdff2f1a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
